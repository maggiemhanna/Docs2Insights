{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14bde9-d21d-48bf-a194-f37eadddee23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927804bd-9675-4ab5-b4f2-dd3bd0c2a1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"arshkon/linkedin-job-postings\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888dbba-0f8b-4756-94c9-eef6672fae06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "shutil.copytree(path, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b062117-3e82-475d-9bac-b9bb90b5d2f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cb378-7643-49d1-b4d0-00c02a698310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all datasets\n",
    "postings = pd.read_csv('data/postings.csv')\n",
    "benefits = pd.read_csv('data/jobs/benefits.csv')\n",
    "salaries = pd.read_csv('data/jobs/salaries.csv')\n",
    "job_industries = pd.read_csv('data/jobs/job_industries.csv')\n",
    "job_skills = pd.read_csv('data/jobs/job_skills.csv')\n",
    "\n",
    "companies = pd.read_csv('data/companies/companies.csv')\n",
    "employee_counts = pd.read_csv('data/companies/employee_counts.csv')\n",
    "company_industries = pd.read_csv('data/companies/company_industries.csv')\n",
    "company_specialities = pd.read_csv('data/companies/company_specialities.csv')\n",
    "\n",
    "# Merge job-related datasets with one-to-one relationships\n",
    "# postings = pd.merge(postings, salaries, on='job_id', how='left')\n",
    "\n",
    "benefits = benefits.groupby('job_id').agg(benefits_types=('type', lambda x: list(x))).reset_index()\n",
    "job_skills = job_skills.groupby('job_id').agg(skills_abreviation=('skill_abr', lambda x: list(x))).reset_index()\n",
    "\n",
    "company_industries = company_industries.groupby('company_id').agg(industries=('industry', lambda x: list(x))).reset_index()\n",
    "company_specialities = company_specialities.groupby('company_id').agg(specialities=('speciality', lambda x: list(x))).reset_index()\n",
    "employee_counts = employee_counts.groupby('company_id').apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "postings = pd.merge(postings, benefits, on='job_id', how='left')\n",
    "postings = pd.merge(postings, job_skills, on='job_id', how='left')\n",
    "\n",
    "# Merge company-related datasets with one-to-one relationships\n",
    "companies = pd.merge(companies, employee_counts, on='company_id', how='left')\n",
    "companies = pd.merge(companies, company_industries, on='company_id', how='left')\n",
    "companies = pd.merge(companies, company_specialities, on='company_id', how='left')\n",
    "\n",
    "# Merge jobs and companies datasets\n",
    "postings = pd.merge(postings, companies, on='company_id', how='left', suffixes=(\"\", \"_company\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3c3f3-0dc4-459b-9342-eb63bc279b55",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f61fba-efad-43db-bd4c-85b80a8755ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import vertexai\n",
    "from vertexai.preview.batch_prediction import BatchPredictionJob\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def upload_to_gcs(bucket_name, blob_name, content, content_type):\n",
    "    \"\"\"Uploads content to Google Cloud Storage as a string.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_string(content, content_type=content_type)\n",
    "    print(f\"File uploaded to {bucket_name}/{blob_name}\")\n",
    "\n",
    "def prepare_jsonl(job_info_list, bucket_name, blob_name):\n",
    "    \"\"\"Prepares and uploads a JSONL file with prompts to Google Cloud Storage.\"\"\"\n",
    "    jsonl_content = \"\"\n",
    "    for job_info in job_info_list:\n",
    "        prompt = \"Generate a well-formatted HTML page for a job listing with the following details:\\n\\n\"\n",
    "        for key, value in job_info.items():\n",
    "            if value:\n",
    "                prompt += f\"{key.replace('_', ' ').capitalize()}: {value}\\n\"\n",
    "        prompt += \"\\nMake sure the HTML page is well-structured and visually appealing.\"\n",
    "        prompt += \"\\nIn your response, make sure you generate the HTML page and nothing else.\"\n",
    "\n",
    "        # Format as JSON for Gemini Batch API\n",
    "        jsonl_content += json.dumps({\n",
    "            \"request\": {\n",
    "                \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
    "            }\n",
    "        }) + \"\\n\"\n",
    "\n",
    "    # Upload JSONL file to GCS\n",
    "    upload_to_gcs(bucket_name, blob_name, jsonl_content, 'application/jsonl')\n",
    "\n",
    "def batch_predict_gemini_createjob(input_uri: str, output_uri: str, model) -> BatchPredictionJob:\n",
    "    \"\"\"Perform batch text prediction using a Gemini AI model.\n",
    "    Args:\n",
    "        input_uri (str): URI of the input file in BigQuery table or Google Cloud Storage.\n",
    "            Example: \"gs://[BUCKET]/[DATASET].jsonl\" OR \"bq://[PROJECT].[DATASET].[TABLE]\"\n",
    "\n",
    "        output_uri (str): URI of the output folder,  in BigQuery table or Google Cloud Storage.\n",
    "            Example: \"gs://[BUCKET]/[OUTPUT].jsonl\" OR \"bq://[PROJECT].[DATASET].[TABLE]\"\n",
    "    Returns:\n",
    "        batch_prediction_job: The batch prediction job object containing details of the job.\n",
    "    \"\"\"\n",
    "\n",
    "    # Submit a batch prediction job with Gemini model\n",
    "    batch_prediction_job = BatchPredictionJob.submit(\n",
    "        source_model=model,\n",
    "        input_dataset=input_uri,\n",
    "        output_uri_prefix=output_uri,\n",
    "    )\n",
    "\n",
    "    # Check job status\n",
    "    print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "    print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "    print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "    # Refresh the job until complete\n",
    "    while not batch_prediction_job.has_ended:\n",
    "        time.sleep(5)\n",
    "        batch_prediction_job.refresh()\n",
    "\n",
    "    # Check if the job succeeds\n",
    "    if batch_prediction_job.has_succeeded:\n",
    "        print(\"Job succeeded!\")\n",
    "    else:\n",
    "        print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "    # Check the location of the output\n",
    "    print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n",
    "    return batch_prediction_job\n",
    "\n",
    "\n",
    "def load_output_and_upload_html(bucket_name, folder_name):\n",
    "    \"\"\"Loads JSONL results from GCS and uploads each HTML result to GCS.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "    \n",
    "    for blob in blobs:\n",
    "        output_content = blob.download_as_string().decode(\"utf-8\")\n",
    "        for line in output_content.strip().split('\\n'):\n",
    "            response_data = json.loads(line)\n",
    "            html_content = response_data[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            job_id = random.randint(10**6, 10**7)\n",
    "            destination_blob_name = f\"data/job_listing_{job_id}.html\"\n",
    "            upload_to_gcs(bucket_name, destination_blob_name, html_content, 'text/html')\n",
    "\n",
    "def main():\n",
    "    \n",
    "    PROJECT_ID = \"vertexai-explore-437408\"\n",
    "    LOCATION = \"us-central1\"\n",
    "    BUCKET_NAME = \"job-listings-data-light\"\n",
    "    MODEL_NAME = \"gemini-1.5-flash-002\"\n",
    "\n",
    "    input_blob = \"batch_input/job_listings.jsonl\"\n",
    "    output_folder = \"batch_output/\"\n",
    "\n",
    "    # Prepare job listings JSONL\n",
    "    job_info = postings.iloc[:20].to_dict(orient='records')\n",
    "    prepare_jsonl(job_info, BUCKET_NAME, input_blob)\n",
    "    \n",
    "    # Create batch prediction job\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    input_uri = f\"gs://{BUCKET_NAME}/{input_blob}\"\n",
    "    output_uri = f\"gs://{BUCKET_NAME}/{output_folder}\"\n",
    "    output_location = batch_predict_gemini_createjob(input_uri, output_uri, MODEL_NAME).output_location\n",
    "    output_location = output_location.replace(f\"gs://{BUCKET_NAME}/\", \"\")\n",
    "\n",
    "    # Load output and upload HTML pages\n",
    "    load_output_and_upload_html(BUCKET_NAME, output_location)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2971f0-18f7-4115-be17-1e8266992907",
   "metadata": {},
   "source": [
    "# Extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10030a-5b9b-418f-96aa-a9519556c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_objects_in_bucket(bucket_name):\n",
    "    # Initialize a Google Cloud Storage client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # List all objects in the bucket and count them\n",
    "    blobs = bucket.list_blobs()\n",
    "    object_count = sum(1 for _ in blobs)\n",
    "    \n",
    "    print(f\"Total number of objects in bucket '{bucket_name}': {object_count}\")\n",
    "    return object_count\n",
    "\n",
    "count_objects_in_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eb8d6-32bf-409b-8075-c8dd29a17c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket_contents(bucket_name):\n",
    "    # Initialize a client\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    # Delete each object\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "        print(f'Deleted {blob.name}')\n",
    "    \n",
    "    print(f'All contents of bucket {bucket_name} have been deleted.')\n",
    "\n",
    "delete_bucket_contents(BUCKET_NAME)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
