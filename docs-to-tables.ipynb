{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeab97-302d-4c6b-98bd-1374435c40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from typing import List, Dict, Any, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import operator\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2740c30-fc9c-4379-8693-f56a972689ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from src.bigquery_utils import get_bigquery_table_schema, create_or_update_table, insert_json_to_bigquery, get_random_rows_from_bigquery\n",
    "from src.schema_utils import generate_schema, parse_json, transform_schema_to_response_format\n",
    "from src.extraction_utils import extract_key_values_from_document\n",
    "from src.storage_utils import list_files_in_bucket  \n",
    "\n",
    "# Importing config variables from config.py\n",
    "from config.config import PROJECT_ID, DATASET_ID, TABLE_ID, BUCKET_NAME, FOLDER_PREFIX, MODEL_NAME, VERTEXAI_LOCATION\n",
    "    \n",
    "vertexai.init(project=PROJECT_ID, location=VERTEXAI_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc733286-28b4-426d-ac9e-d7cded9e02c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List files in the bucket\n",
    "MAX_FILES = 300\n",
    "file_uris = list_files_in_bucket(BUCKET_NAME, FOLDER_PREFIX, MAX_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811b378-c8db-4be1-a625-f0a483e2930c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    project_id: str\n",
    "    dataset_id: str\n",
    "    table_id: str\n",
    "    file_uri: str\n",
    "    model_name: str\n",
    "    \n",
    "class OutputState(TypedDict):\n",
    "    result: str\n",
    "    \n",
    "# Define the overall schema, combining both input and output\n",
    "class OverallState(InputState, OutputState):\n",
    "    existing_schema: List[Dict[str, Any]]\n",
    "    new_schema: List[Dict[str, Any]]\n",
    "    new_schema_bq: List[Dict[str, Any]]\n",
    "    response_schema: Dict[str, Any]\n",
    "    random_rows: List[Dict[str, Any]]\n",
    "    key_values: List[Dict[str, Any]]\n",
    "    table_update_result: str\n",
    "    \n",
    "    \n",
    "# Node 1: Get existing BigQuery table schema\n",
    "def get_bigquery_table_schema_(state: InputState) -> OverallState:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "     \n",
    "    return {\"existing_schema\": get_bigquery_table_schema(project_id, dataset_id, table_id)}\n",
    "\n",
    "# Node 2: Generate new BigQuery schema\n",
    "def generate_schema_(state: OverallState) -> OverallState:\n",
    "    file_uri = state[\"file_uri\"]\n",
    "    model_name = state[\"model_name\"]\n",
    "    existing_schema = state[\"existing_schema\"]\n",
    "    \n",
    "    return {\"new_schema\": generate_schema(file_uri, existing_schema, model_name)}\n",
    "\n",
    "\n",
    "# Node 3: Create or update BigQuery table\n",
    "def create_or_update_table_(state: OverallState) -> OverallState:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "    new_schema = state[\"new_schema\"]\n",
    "\n",
    "    return {\"table_update_result\": create_or_update_table(dataset_id, table_id, new_schema, project_id)}\n",
    "\n",
    "# Node 4: Get the updated BigQuery table schema\n",
    "def get_updated_bigquery_table_schema_(state: OverallState) -> OverallState:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "\n",
    "    return {\"new_schema_bq\": get_bigquery_table_schema(project_id, dataset_id, table_id)}\n",
    "\n",
    "# Node 5: Transform schema to response format\n",
    "def transform_schema_to_response_format_(state: OverallState) -> OverallState:\n",
    "    schema = state[\"new_schema_bq\"]\n",
    "\n",
    "    return {\"response_schema\": transform_schema_to_response_format(schema)}\n",
    "\n",
    "# Node 6: Get random rows from BigQuery table\n",
    "def get_random_rows_(state: OverallState) -> OverallState:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "    num_rows = 6\n",
    "    \n",
    "    return {\"random_rows\": get_random_rows_from_bigquery(project_id, dataset_id, table_id, num_rows)}\n",
    "\n",
    "# Node 7: Extract key-value pairs from the document\n",
    "def extract_key_values_(state: OverallState) -> OverallState:\n",
    "    file_uri = state[\"file_uri\"]\n",
    "    model_name = state[\"model_name\"]\n",
    "    schema = state[\"new_schema_bq\"]\n",
    "    response_schema = state[\"response_schema\"]\n",
    "    random_rows = state[\"random_rows\"]\n",
    "        \n",
    "    return {\"key_values\": extract_key_values_from_document(file_uri, schema, random_rows, response_schema, model_name)}\n",
    "\n",
    "# Node 8: Insert extracted key-value pairs into BigQuery table\n",
    "def insert_json_to_bigquery_(state: OverallState) -> OutputState:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "    json_data = state[\"key_values\"]\n",
    "    \n",
    "    return {\"result\": insert_json_to_bigquery(project_id, dataset_id, table_id, json_data)}\n",
    "\n",
    "\n",
    "def table_update_result(state: OverallState):\n",
    "    result = state[\"table_update_result\"]\n",
    "    if result == \"Error\":\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        return \"success\"\n",
    "\n",
    "def table_insert_result(state: OverallState):\n",
    "    result = state[\"result\"]\n",
    "    if result == \"Error\":\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        return \"success\"\n",
    "\n",
    "# Build the graph with input and output schemas specified\n",
    "graph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "# Adding nodes\n",
    "graph.add_node(get_bigquery_table_schema_)\n",
    "graph.add_node(generate_schema_)\n",
    "graph.add_node(create_or_update_table_)\n",
    "graph.add_node(get_updated_bigquery_table_schema_)\n",
    "graph.add_node(transform_schema_to_response_format_)\n",
    "graph.add_node(get_random_rows_)\n",
    "graph.add_node(extract_key_values_)\n",
    "graph.add_node(insert_json_to_bigquery_)\n",
    "\n",
    "graph.add_edge(START, \"get_bigquery_table_schema_\")  \n",
    "graph.add_edge(\"get_bigquery_table_schema_\", \"generate_schema_\")  \n",
    "graph.add_edge(\"generate_schema_\", \"create_or_update_table_\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"create_or_update_table_\",\n",
    "    table_update_result,\n",
    "    {\n",
    "        \"retry\": \"generate_schema_\",\n",
    "        \"success\": \"get_updated_bigquery_table_schema_\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"get_updated_bigquery_table_schema_\", \"transform_schema_to_response_format_\")\n",
    "graph.add_edge(START, \"get_random_rows_\")\n",
    "graph.add_edge([\"get_random_rows_\", \"transform_schema_to_response_format_\"], \"extract_key_values_\")\n",
    "graph.add_edge(\"extract_key_values_\", \"insert_json_to_bigquery_\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"insert_json_to_bigquery_\",\n",
    "    table_insert_result,\n",
    "\n",
    "    {\n",
    "        \"retry\": \"extract_key_values_\",\n",
    "        \"success\": END\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)  # Compile the graph\n",
    "\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40630a40-1805-46ab-b914-b8b6fe570499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failed_uris = []  # List to store failed file_uris\n",
    "\n",
    "for file_uri in file_uris[:30]:\n",
    "    print(\"\\n\")\n",
    "    print(f\"{'=' * 10} {file_uri} {'=' * (102 - len(file_uri))}\")\n",
    "    config = {\"configurable\": {\"thread_id\": file_uri, \"recursion_limit\": 50}}\n",
    "\n",
    "    try:\n",
    "        app.invoke(\n",
    "            {\n",
    "                \"project_id\": PROJECT_ID,\n",
    "                \"dataset_id\": DATASET_ID,\n",
    "                \"table_id\": TABLE_ID,\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"file_uri\": file_uri,\n",
    "            },\n",
    "            config,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_uri}: {e}\")\n",
    "        failed_uris.append(file_uri)\n",
    "\n",
    "# You can treat the failed_uris list later\n",
    "print(\"Failed URIs to treat later:\", failed_uris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62601e-bc71-4cb0-9f5c-6672770cf8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bf9c1-5d3b-4ab3-b3f6-9fd70e5de1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": file_uri}}\n",
    "app.get_state(config).values[\"existing_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e82b8-d7c4-454c-9e68-771fdb17842f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app.get_state(config).values[\"new_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227e83-a527-431c-b13f-3519e7a8080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import List, Dict, Any, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.types import Send\n",
    "import operator\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "class InputStateSubset(TypedDict):\n",
    "    project_id: str\n",
    "    dataset_id: str\n",
    "    table_id: str\n",
    "    file_uri: str\n",
    "    model_name: str\n",
    "    response_schema: Dict[str, Any]\n",
    "    random_rows: List[Dict[str, Any]]\n",
    "\n",
    "class OutputStateSubset(TypedDict):\n",
    "    result: str\n",
    "    \n",
    "# Define the overall schema, combining both input and output\n",
    "class OverallStateSubset(InputStateSubset, OutputStateSubset):\n",
    "    key_values: List[Dict[str, Any]]\n",
    "\n",
    "# Build the graph with input and output schemas specified\n",
    "graph_subset = StateGraph(OverallStateSubset, input=InputStateSubset, output=OutputStateSubset)\n",
    "\n",
    "graph_subset.add_edge(START, \"extract_key_values_\")\n",
    "graph_subset.add_edge(\"extract_key_values_\", \"insert_json_to_bigquery_\")\n",
    "graph_subset.add_conditional_edges(\n",
    "    \"insert_json_to_bigquery_\",\n",
    "    table_insert_result,\n",
    "\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"retry\": \"extract_key_values_\",\n",
    "        # We may ask the human\n",
    "        \"success\": END\n",
    "    },\n",
    ")\n",
    "\n",
    "app_subset = graph_subset.compile()  # Compile the graph\n",
    "\n",
    "\n",
    "try:\n",
    "    display(Image(app_subset.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c6f97-bf64-4983-9362-d509aa35c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values[\"response_schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296e342-1092-4713-8f57-fa046057c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values[\"random_rows\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ab3d7-695f-4d98-bf84-b4e21b05c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uri = file_uris[10]\n",
    "\n",
    "app_subset.invoke({\"project_id\": PROJECT_ID, \"dataset_id\": DATASET_ID, \"table_id\": TABLE_ID, \"model_name\": MODEL_NAME, \"file_uri\" : file_uri, \"response_schema\": response_schema, \"random_rows\": random_rows})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d55b75-03ca-440f-8d85-37e0ce40a0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputStateBatch(TypedDict):\n",
    "    project_id: str\n",
    "    dataset_id: str\n",
    "    table_id: str\n",
    "    model_name: str\n",
    "    file_uris: List[str]\n",
    "    \n",
    "class OutputStateBatch(TypedDict):\n",
    "    results: Annotated[List, operator.add]\n",
    "\n",
    "# Define the overall schema, combining both input and output\n",
    "class OverallStateBatch(InputStateBatch, OutputStateBatch):\n",
    "    file_uri: str\n",
    "    \n",
    "def extract_and_insert(state: OverallStateBatch) -> OverallStateBatch:\n",
    "    project_id = state[\"project_id\"]\n",
    "    dataset_id = state[\"dataset_id\"]\n",
    "    table_id = state[\"table_id\"]\n",
    "    model_name = state[\"model_name\"]\n",
    "    file_uri = state[\"uri\"]\n",
    "    result = app.invoke({\"project_id\": project_id, \n",
    "                                \"dataset_id\": dataset_id, \n",
    "                                \"table_id\": table_id, \n",
    "                                \"model_name\": model_name, \n",
    "                                \"file_uri\" : file_uri})\n",
    "    \n",
    "    return {\"results\": [result]}\n",
    "\n",
    "def continue_to_insert(state: OverallStateBatch):\n",
    "    # We will return a list of `Send` objects\n",
    "    # Each `Send` object consists of the name of a node in the graph\n",
    "    # as well as the state to send to that nod    \n",
    "    \n",
    "    for uri in state[\"file_uris\"]:\n",
    "        state[\"uri\"] = uri\n",
    "        state[\"results\"].append(Send(\"extract_and_insert\", state))\n",
    "    \n",
    "    return state[\"results\"]\n",
    "\n",
    "\n",
    "graph_batch = StateGraph(OverallStateBatch, input=InputStateBatch, output=OutputStateBatch)\n",
    "\n",
    "graph_batch.add_node(\"extract_and_insert\", extract_and_insert)\n",
    "graph_batch.add_conditional_edges(START, continue_to_insert, [\"extract_and_insert\"])\n",
    "graph_batch.add_edge(\"extract_and_insert\", END)\n",
    "\n",
    "app_batch = graph_batch.compile()\n",
    "\n",
    "Image(app_batch.get_graph().draw_mermaid_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65849ceb-8460-48ff-977e-bb2852dccad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_batch.invoke({\"project_id\": PROJECT_ID, \"dataset_id\": DATASET_ID, \"table_id\": TABLE_ID, \"model_name\": MODEL_NAME, \"file_uris\" : file_uris[:30]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751186ce-5e74-4006-a422-a20b959af2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
